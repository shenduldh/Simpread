> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [spaces.ac.cn](https://spaces.ac.cn/archives/9009)

> Pre Norm 与 Post Norm 之间的对比是一个 “老生常谈” 的话题了，本博客就多次讨论过这个问题，比如文章《浅谈 Transformer 的初始化、参数化与标准化》、《模型优化漫谈：BERT 的初...

Pre Norm 与 Post Norm 之间的对比是一个 “老生常谈” 的话题了，本博客就多次讨论过这个问题，比如文章[《浅谈 Transformer 的初始化、参数化与标准化》](/archives/8620)、[《模型优化漫谈：BERT 的初始标准差为什么是 0.02？》](/archives/8747)等。目前比较明确的结论是：同一设置之下，Pre Norm 结构往往更容易训练，但最终效果通常不如 Post Norm。Pre Norm 更容易训练好理解，因为它的恒等路径更突出，但为什么它效果反而没那么好呢？

笔者之前也一直没有好的答案，直到前些时间在知乎上看到 [@唐翔昊](https://www.zhihu.com/question/519668254/answer/2371885202) 的一个回复后才 “恍然大悟”，原来这个问题竟然有一个非常直观的理解！本文让我们一起来学习一下。

基本结论 [#](#基本结论)
---------------

Pre Norm 和 Post Norm 的式子分别如下：  

$$\begin{align} \text{Pre Norm: } \quad \boldsymbol{x}_{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))\\ \text{Post Norm: }\quad \boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x}_t)) \end{align}$$

在 Transformer 中，这里的

$\text{Norm}$

主要指 Layer Normalization，但在一般的模型中，它也可以是 Batch Normalization、Instance Normalization 等，相关结论本质上是通用的。

在笔者找到的资料中，显示 Post Norm 优于 Pre Norm 的工作有两篇，一篇是[《Understanding the Difficulty of Training Transformers》](https://arxiv.org/abs/2004.08249)，一篇是[《RealFormer: Transformer Likes Residual Attention》](https://arxiv.org/abs/2012.11747)。另外，笔者自己也做过对比实验，显示 Post Norm 的结构迁移性能更加好，也就是说在 Pretraining 中，Pre Norm 和 Post Norm 都能做到大致相同的结果，但是 Post Norm 的 Finetune 效果明显更好。

可能读者会反问[《On Layer Normalization in the Transformer Architecture》](https://arxiv.org/abs/2002.04745)不是显示 Pre Norm 要好于 Post Norm 吗？这是不是矛盾了？其实这篇文章比较的是在完全相同的训练设置下 Pre Norm 的效果要优于 Post Norm，这只能显示出 Pre Norm 更容易训练，因为 Post Norm 要达到自己的最优效果，不能用跟 Pre Norm 一样的训练配置（比如 Pre Norm 可以不加 Warmup 但 Post Norm 通常要加），所以结论并不矛盾。

直观理解 [#](#直观理解)
---------------

为什么 Pre Norm 的效果不如 Post Norm？知乎上 [@唐翔昊](https://www.zhihu.com/question/519668254/answer/2371885202) 给出的答案是：Pre Norm 的深度有 “水分”！也就是说，一个 $L$层的 Pre Norm 模型，其实际等效层数不如 $L$层的 Post Norm 模型，而层数少了导致效果变差了。

具体怎么理解呢？很简单，对于 Pre Norm 模型我们迭代得到：  

$$\begin{equation}\begin{aligned} \boldsymbol{x}_{t+1} =&\,\boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t)) \\ =&\, \boldsymbol{x}_{t-1} + F_{t-1}(\text{Norm}(\boldsymbol{x}_{t-1})) + F_t(\text{Norm}(\boldsymbol{x}_t)) \\ =&\, \cdots \\ =&\, \boldsymbol{x}_0 + F_0 ((\text{Norm}(\boldsymbol{x}_0)) + \cdots + F_{t-1}(\text{Norm}(\boldsymbol{x}_{t-1})) + F_t(\text{Norm}(\boldsymbol{x}_t)) \end{aligned}\end{equation}$$

其中每一项都是同一量级的，那么有

$\boldsymbol{x}_{t+1}=\mathscr{O}(t+1)$

，也就是说第

$t+1$

层跟第

$t$

层的差别就相当于

$t+1$

与

$t$

的差别，当

$t$

较大时，两者的相对差别是很小的，因此

$$\begin{equation}\begin{aligned} &\,F_t(\text{Norm}(\boldsymbol{x}_t)) + F_{t+1}(\text{Norm}(\boldsymbol{x}_{t+1})) \\ \approx&\,F_t(\text{Norm}(\boldsymbol{x}_t)) + F_{t+1}(\text{Norm}(\boldsymbol{x}_t)) \\ =&\, (F_t\oplus F_{t+1})(\text{Norm}(\boldsymbol{x}_t)) \end{aligned}\end{equation}$$

这个意思是说，当

$t$

比较大时，

$\boldsymbol{x}_t,\boldsymbol{x}_{t+1}$

相差较小，所以

$F_{t+1}(\text{Norm}(\boldsymbol{x}_{t+1}))$

与

$F_{t+1}(\text{Norm}(\boldsymbol{x}_t))$

很接近，因此原本一个

$t$

层的模型与

$t+1$

层和，近似等效于一个更宽的

$t$

层模型，所以在 Pre Norm 中多层叠加的结果更多是增加宽度而不是深度，层数越多，这个层就越 “虚”。

说白了，Pre Norm 结构无形地增加了模型的宽度而降低了模型的深度，而我们知道深度通常比宽度更重要，所以是无形之中的降低深度导致最终效果变差了。而 Post Norm 刚刚相反，在[《浅谈 Transformer 的初始化、参数化与标准化》](/archives/8620)中我们就分析过，它每 Norm 一次就削弱一次恒等分支的权重，所以 Post Norm 反而是更突出残差分支的，因此 Post Norm 中的层数更加 “足秤”，一旦训练好之后效果更优。

相关工作 [#](#相关工作)
---------------

前段时间号称能训练 1000 层 Transformer 的 [DeepNet](/archives/8978) 想必不少读者都听说过，在其论文[《DeepNet: Scaling Transformers to 1,000 Layers》](https://arxiv.org/abs/2203.00555)中对 Pre Norm 的描述是：

> However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers, leading to a degradation in performance compared with Post-LN.

不少读者当时可能并不理解这段话的逻辑关系，但看了前一节内容的解释后，想必会有新的理解。

简单来说，所谓 “the gradients of Pre-LN at bottom layers tend to be larger than at top layers”，就是指 Pre Norm 结构会过度倾向于恒等分支（bottom layers），从而使得 Pre Norm 倾向于退化（degradation）为一个“浅而宽” 的模型，最终不如同一深度的 Post Norm。这跟前面的直观理解本质上是一致的。

文章小结 [#](#文章小结)
---------------

本文主要分享了 “为什么 Pre Norm 的效果不如 Post Norm” 的一个直观理解。

_**转载到请包括本文地址：**[https://spaces.ac.cn/archives/9009](https://spaces.ac.cn/archives/9009 "为什么Pre Norm的效果不如Post Norm？")_

_**更详细的转载事宜请参考：**_[《科学空间 FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎[分享](#share) / [打赏](#pay)本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏 ![](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。  
你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 29, 2022). 《为什么 Pre Norm 的效果不如 Post Norm？ 》[Blog post]. Retrieved from [https://spaces.ac.cn/archives/9009](https://spaces.ac.cn/archives/9009)

@online{kexuefm-9009,  
        title={为什么 Pre Norm 的效果不如 Post Norm？},  
        author={苏剑林},  
        year={2022},  
        month={Mar},  
        url={\url{https://spaces.ac.cn/archives/9009}},  
}